# -*- coding: utf-8 -*-
"""train_sigext_longformer_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ip3Ve4COpgEkcmaTp8QepY6qIiqphPtG
"""

!pip -q install -U transformers accelerate datasets evaluate scikit-learn

from google.colab import files
files.upload()   # choose sigext_train.jsonl

import os
assert "sigext_train.jsonl" in os.listdir("."), "sigext_train.jsonl not found"
print("OK")

import json, random

def load_sigext_data(path, max_samples=None):
    data = []
    with open(path) as f:
        for i, line in enumerate(f):
            if max_samples is not None and i >= max_samples:
                break
            data.append(json.loads(line))
    return data

data = load_sigext_data("sigext_train.jsonl")


random.seed(42)


split = int(0.9 * len(data))
train_data = data[:split]
val_data   = data[split:]

print("train:", len(train_data), "val:", len(val_data), "keys:", list(train_data[0].keys()))

from transformers import AutoTokenizer

MODEL_NAME = "allenai/longformer-base-4096"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

MAX_LEN = 4096

def build_examples(data, max_len=MAX_LEN):
    examples = []
    for ex in data:
        article = ex["article"]
        phrases = ex.get("phrases") or ex.get("candidates") or ex.get("keyphrases")
        labels  = ex.get("labels")  or ex.get("phrase_labels") or ex.get("y")
        if phrases is None or labels is None:
            raise KeyError(f"Missing phrases/labels. Keys: {list(ex.keys())}")

        enc = tokenizer(
            article,
            truncation=True,
            padding="max_length",
            max_length=max_len,
            return_offsets_mapping=True,
        )

        offsets = enc["offset_mapping"]
        token_labels = [0] * len(offsets)

        for ph, y in zip(phrases, labels):
            if int(y) != 1:
                continue
            start = article.lower().find(ph.lower())
            if start == -1:
                continue
            end = start + len(ph)
            for i, (s, e) in enumerate(offsets):
                if s == e:
                    continue
                if not (e <= start or s >= end):
                    token_labels[i] = 1

        enc.pop("offset_mapping")
        enc["labels"] = token_labels
        examples.append(enc)
    return examples

mini = build_examples(train_data[:50])
print("Built", len(mini), "pos_tokens_ex0=", sum(mini[0]["labels"]))

train_examples = build_examples(train_data)
len(train_examples)

from datasets import Dataset

train_ds = Dataset.from_list(train_examples)
train_ds

import os
import torch
from transformers import AutoModelForTokenClassification, AutoTokenizer

BASE_MODEL = "allenai/longformer-base-4096"
CKPT_ROOT = "sigext_longformer_ckpt_fp32"

def find_checkpoint(root):
    if os.path.isdir(root):
        for dirpath, _, filenames in os.walk(root):
            if any(f in filenames for f in ["pytorch_model.bin", "model.safetensors"]):
                return dirpath
    raise FileNotFoundError("No checkpoint with weights found")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

ckpt_path = find_checkpoint(CKPT_ROOT)

model = AutoModelForTokenClassification.from_pretrained(ckpt_path)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

print("Loaded from:", ckpt_path)
print("Device:", device)

# model.config.gradient_checkpointing = True
# model.gradient_checkpointing_enable()

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./sigext_longformer_ckpt_fp32",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    weight_decay=0.01,

    logging_steps=50,
    save_steps=250,
    save_total_limit=2,

    report_to="none"
)

from datasets import Dataset
from transformers import AutoTokenizer

# Tokenizer for Longformer
MODEL_NAME = "allenai/longformer-base-4096"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Build HF dataset from loaded JSONL lines (train_data)
raw_ds = Dataset.from_list(train_data)

import re

def _find_spans(text, phrase):
    spans = []
    for m in re.finditer(re.escape(phrase), text, flags=re.IGNORECASE):
        spans.append((m.start(), m.end()))
    return spans

def preprocess_to_token_labels(batch):
    article = batch["article"]
    phrases = batch.get("phrases", [])
    labels  = batch.get("labels", [])

    pos_phrases = [p for p, y in zip(phrases, labels) if int(y) == 1]

    pos_spans = []
    for p in pos_phrases:
        if isinstance(p, str) and p.strip():
            pos_spans.extend(_find_spans(article, p.strip()))

    enc = tokenizer(
        article,
        truncation=True,
        max_length=4096,
        return_offsets_mapping=True
    )

    offsets = enc["offset_mapping"]
    token_labels = []

    for (s, e) in offsets:
        if s == 0 and e == 0:
            token_labels.append(-100)
            continue

        y = 0
        for (ps, pe) in pos_spans:
            if not (e <= ps or s >= pe):
                y = 1
                break
        token_labels.append(y)

    enc.pop("offset_mapping")
    enc["labels"] = token_labels
    return enc

train_dataset = raw_ds.map(preprocess_to_token_labels)

import torch
from transformers import Trainer

class WeightedTrainer(Trainer):
    def compute_loss(
        self,
        model,
        inputs,
        return_outputs=False,
        num_items_in_batch=None,
    ):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        loss_fct = torch.nn.CrossEntropyLoss(
            weight=torch.tensor([1.0, 20.0], device=logits.device),
            ignore_index=-100
        )
        loss = loss_fct(logits.view(-1, 2), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer)

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    data_collator=data_collator,
)

trainer.train()

import shutil

shutil.make_archive(
    "sigext_longformer_ckpt_fp32",
    "zip",
    "sigext_longformer_ckpt_fp32"
)

from google.colab import files
files.download("sigext_longformer_ckpt_fp32.zip")

!pip install -q gdown
!gdown https://drive.google.com/uc?id=10YE1Zeak-VddADLe0rRPyA7ou6UfMWz5

import os
os.listdir("/content")

import zipfile, os

zip_path = "/content/sigext_longformer_ckpt_fp32.zip"
extract_path = "/content/sigext_longformer_ckpt_fp32"

with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall(extract_path)

os.listdir(extract_path)

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification

# load checkpoint
model_path = "./sigext_longformer_ckpt_fp32/checkpoint-675"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForTokenClassification.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")
model.eval()

import json

# load eval data
with open("sigext_train.jsonl") as f:
    eval_data = [json.loads(line) for line in f][:50]  # sample 50 examples

def extract_keyphrases(text, model, tokenizer, threshold=0.5):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=4096)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs).logits
        probs = torch.softmax(outputs, dim=-1)[0, :, 1]
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    selected = [t for t, p in zip(tokens, probs) if p > threshold and t not in tokenizer.all_special_tokens]
    return tokenizer.convert_tokens_to_string(selected)

for i, ex in enumerate(eval_data[:3]):
    article = ex["article"]
    summary = ex.get("summary", "")
    pred = extract_keyphrases(article, model, tokenizer, threshold=0.45)
    print(f"Example {i}\nSummary: {summary}\nPred: {pred}\n{'-'*60}")

train_ds[0].keys()

from sklearn.metrics import precision_recall_fscore_support

def normalize(ph):
    return ph.lower().strip()

def eval_example(pred, gold):
    pred = set(map(normalize, pred))
    gold = set(map(normalize, gold))

    tp = len(pred & gold)
    fp = len(pred - gold)
    fn = len(gold - pred)

    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1 = 2 * precision * recall / (precision + recall + 1e-8)
    return precision, recall, f1

def get_gold_phrases(ex):
    return [p for p, y in zip(ex["phrases"], ex["labels"]) if int(y) == 1]

# === FINAL FIXED INFERENCE + EVAL CELL (NO ARGUMENT ERRORS) ===

import torch, json
from transformers import AutoTokenizer, AutoModelForTokenClassification

CKPT = "./sigext_longformer_ckpt_fp32/checkpoint-675"
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(CKPT)
model = AutoModelForTokenClassification.from_pretrained(CKPT).to(device)
model.eval()

with open("sigext_train.jsonl") as f:
    eval_data = [json.loads(line) for line in f][:20]

def extract_keyphrases(text, threshold=0.45):
    enc = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=4096
    )
    enc = {k: v.to(device) for k, v in enc.items()}

    with torch.no_grad():
        logits = model(**enc).logits
        probs = torch.softmax(logits, dim=-1)[0, :, 1]

    tokens = tokenizer.convert_ids_to_tokens(enc["input_ids"][0])
    phrases = [
        t for t, p in zip(tokens, probs)
        if p > threshold and t not in tokenizer.all_special_tokens
    ]
    return tokenizer.convert_tokens_to_string(phrases)

def normalize(x):
    return x.lower().strip()

def get_gold_phrases(ex):
    return [p for p, y in zip(ex["phrases"], ex["labels"]) if int(y) == 1]

def eval_example(pred_phrases, gold_phrases):
    pred = set(map(normalize, pred_phrases))
    gold = set(map(normalize, gold_phrases))

    tp = len(pred & gold)
    fp = len(pred - gold)
    fn = len(gold - pred)

    precision = tp / (tp + fp + 1e-8)
    recall    = tp / (tp + fn + 1e-8)
    f1        = 2 * precision * recall / (precision + recall + 1e-8)
    return precision, recall, f1

for i, ex in enumerate(eval_data[:5]):
    gold = get_gold_phrases(ex)
    pred = extract_keyphrases(ex["article"])

    p, r, f1 = eval_example(pred.split(), gold)

    print(f"\nExample {i}")
    print("Gold:", gold[:10])
    print("Pred:", pred)
    print(f"P={p:.3f} R={r:.3f} F1={f1:.3f}")

!ls

from google.colab import files

files.download("sigext_train.jsonl")
files.download("sigext_longformer_ckpt_fp32.zip")